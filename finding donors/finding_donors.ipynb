{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding_donors for charity ML:\n",
    "\n",
    "The goal here is to implement supervised learners on the census data set provided to determine which algorithm will provide the highest donation yield for a fictitious charity organization located in the Silicon Valley. Census data contains records of individuals with additional details like age, workclass, education_level, education_num, marital status and so on. \n",
    "To begin with we start with data exploration, data preprocessing then model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration:\n",
    "\n",
    "A machine learning algorithm usually takes clean (and often tabular) data, and learns some pattern in the data, to make predictions on new data. However, when ML is used in real-world applications, the raw information that you get from the real-world is often not ready to be fed into the ML algorithm. So you need to preprocess that information to create input data for the ML algorithm. \n",
    "\n",
    "Using the census.csv dataset we will segregate the individuals into different groups based on the income criteria as \n",
    "1.total number of individuals\n",
    "2.individuals with income more than \\$50,000 annually, count_greater_50k\n",
    "3.individuals with income at most \\$50,000 annually, count_at_most_50k\n",
    "4.percentage of individuals income more than \\$50,000 annually, greater_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_level</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass education_level  education-num       marital-status  \\\n",
       "0   39          State-gov       Bachelors           13.0        Never-married   \n",
       "1   50   Self-emp-not-inc       Bachelors           13.0   Married-civ-spouse   \n",
       "2   38            Private         HS-grad            9.0             Divorced   \n",
       "3   53            Private            11th            7.0   Married-civ-spouse   \n",
       "4   28            Private       Bachelors           13.0   Married-civ-spouse   \n",
       "\n",
       "           occupation    relationship    race      sex  capital-gain  \\\n",
       "0        Adm-clerical   Not-in-family   White     Male        2174.0   \n",
       "1     Exec-managerial         Husband   White     Male           0.0   \n",
       "2   Handlers-cleaners   Not-in-family   White     Male           0.0   \n",
       "3   Handlers-cleaners         Husband   Black     Male           0.0   \n",
       "4      Prof-specialty            Wife   Black   Female           0.0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country income  \n",
       "0           0.0            40.0   United-States  <=50K  \n",
       "1           0.0            13.0   United-States  <=50K  \n",
       "2           0.0            40.0   United-States  <=50K  \n",
       "3           0.0            40.0   United-States  <=50K  \n",
       "4           0.0            40.0            Cuba  <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries numpy, pandas, time, display, matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Import supplementary visualization code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the Census dataset\n",
    "data = pd.read_csv(\"census.csv\")\n",
    "\n",
    "# Success - Display the first record\n",
    "display(data.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 45222\n",
      "Individuals with income greater than $50,000: 11208\n",
      "Individuals making at most $50,000: 34014\n",
      "Percentage of individuals with income greater than $50,000: 24.78%\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration \n",
    "# TODO: Total number of records\n",
    "count_records = data['age'].count( )\n",
    "\n",
    "# TODO: Number of records where individual's income is greater than $50,000\n",
    "count_greater_50k = data[data.income==\">50K\"].income.count()\n",
    "\n",
    "# TODO: Number of records where individual's income is less than or equal to $50,000\n",
    "count_at_most_50k = data[data.income==\"<=50K\"].income.count()\n",
    "\n",
    "# TODO: Percentage of individuals whose income is more than $50,000\n",
    "greater_percent = float(count_greater_50k)*100/count_records\n",
    "\n",
    "# Print the results\n",
    "\n",
    "\n",
    "print (\"Total number of records: {}\".format(count_records))\n",
    "print (\"Individuals with income greater than $50,000: {}\".format(count_greater_50k))\n",
    "print (\"Individuals making at most $50,000: {}\".format(count_at_most_50k))\n",
    "print (\"Percentage of individuals with income greater than $50,000: {:.2f}%\".format(greater_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_level</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass education_level  education-num       marital-status  \\\n",
       "0   39          State-gov       Bachelors           13.0        Never-married   \n",
       "1   50   Self-emp-not-inc       Bachelors           13.0   Married-civ-spouse   \n",
       "2   38            Private         HS-grad            9.0             Divorced   \n",
       "3   53            Private            11th            7.0   Married-civ-spouse   \n",
       "4   28            Private       Bachelors           13.0   Married-civ-spouse   \n",
       "\n",
       "           occupation    relationship    race      sex  capital-gain  \\\n",
       "0        Adm-clerical   Not-in-family   White     Male        2174.0   \n",
       "1     Exec-managerial         Husband   White     Male           0.0   \n",
       "2   Handlers-cleaners   Not-in-family   White     Male           0.0   \n",
       "3   Handlers-cleaners         Husband   Black     Male           0.0   \n",
       "4      Prof-specialty            Wife   Black   Female           0.0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country  \n",
       "0           0.0            40.0   United-States  \n",
       "1           0.0            13.0   United-States  \n",
       "2           0.0            40.0   United-States  \n",
       "3           0.0            40.0   United-States  \n",
       "4           0.0            40.0            Cuba  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAF2CAYAAAD+y36TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3XmYLFV9//H3h1VERVRABBQlxiXG\nBRAxGgQXRFyIW4IRubgbNdGoP8UVxF0jCjFuUQSXuKEiIoqIgiuyiYALiwJ6ZRUUWQQEzu+Pc5rb\nt+mZqbl3eqZn+v16nn6m69TpqlNVPXX6W+fUqZRSkCRJkqQu1ljoAkiSJElaPAwgJEmSJHVmACFJ\nkiSpMwMISZIkSZ0ZQEiSJEnqzABCkiRJUmcGEFpQSf4pyfeSXJLkL0nOT3JYkl368uyVpCT5m4Us\n66rqK/+WM+Q7uOUrSW5KckWSXyT5eJKHrupyh3zmObMs/8FJzuub3rKt93mzWc6qlGtVtnGcJFkj\nyfuTXNiO6WEz5F8/yWuTnJLkyiTXJjkzyQdG+f1Psm+SRw5JX+nYL3VJ7tr29dlt31+V5MQkr0+y\nwUKXb1T6zjslyV+TXJrk+0nemGTj1Vju0O/VapZ134Hy9r9G8j+yKudNaalba6ELoMmV5D+AA4CD\ngPcAVwNbAY8HHgl8c+FKt2AuBZ7U3q8P3AvYA/hRkneWUl7bl/frwEOBC2ex/L2o//cHzeIzb6Ee\np1Hai+HlWpVtHCdPA14GvBL4MXDZVBmTbAp8G7gL8AHgB8D1wH2B5wAPAx40onLuA7wN+M5A+nwc\n+7GQZAfgcOAS4EDgDGBtYHvgJcCdgP9csAKO3sHAR6gXFu9I3e5/B/4jyW6llB+twjKn+l7NhYcD\nNw6k/W4E64FVO29KS5oBhBbSq4DDSinP7Uv7DvC/SSa1dez6UsrxfdPHJPkQ8D5g7yQnlVK+BFBK\nuZQacIxEknVLKdeVUn49qnXMZNTbOA/u0/6+v5Ry0wx5PwVsCmxXSjm7L/27ST4I7DaKAk5nIY/9\nfEqyIXAo8Evg0aWUq/tmfyvJe4F/WJDCzZ/fD5x7vpbkQOD7wJeTbDWwXxbaT0opNyx0IVZVkrWB\nG4pP89UiNak/0jQe7gBcNGzGTD+2kmyT5OIkX05yq5a2Vuv+8ask1yW5IMl7e/NbnjOSfKxveoMk\nNyZZPrD8Hyb5Qt/0jMtu+e6R5OtJrmndAA4A1p3NThmyLwrwauBi4OV967pF954k/5rkp63rxRVJ\nTk/ywjbvWOARwMP6mvyPHVjWDkm+mORPwE/avKm6sayTZP/U7mfXJDlisKtRW+a+A2m9LlB7zaJc\n/du4dpK3JjkvyfXt71tbhTy4jhcm2S+1C9GfknwtyeYD5Zlyn00nyS5Jfpza9e6K1K539+qbfx7Q\n2/Yb+7d5yLK2Ax4FvH0geADqd6CUclhf/jnbB0l6P2Be37f/923zpuq+1mW/znjs+9L3SPKz1G5D\nf0jyqdQWmVkvL8mDkxyd5LL2vfxNagA2necDGwH/PuxHcinl6lLK0X3ruHWSdyU5t+3/c1O7Oa3R\nl2fHVrYnpXaL+kPqOeHTSW4/sB0vS/LL9l36Y5KTkjy5b/55SQ4eLNfgPknyt0m+0v4nr03y29T/\n51W6WFhKuRj4f8AmwO5969k5yZHt+F+Tel59ZZI1+8vW3g77Xj04yaFJlrdtPjPJ25OstyrlHCbJ\n3ZN8pu3z65Kc2r9PW56/ad+1c1s5fpPkQ6kBZS/PsUx9ftq3bzv7lzvV/82Lk7w7yQXAdcDtZ1HW\nOT220uryi6eFdAKwLMlvgK+WUs7q8qEkOwNfAj4DvKSU0mvG/jTwROBdwI+oV3/fAmwJPLXl+Q7w\nhL7F7Ug9kW+W5G9LKWclWR94cFtez4zLTrIOcDSwHrXLwyXAC4GndNmu6ZRSrk9yDPC0JGsNu/KW\n5OGtnAdSK/01gHvTKingxW3+mq1cAH8eWMxngM9Su97MdH54LXAq8GxgY+Dt1Ku1f1dK+essNq9L\nufodAvxzW98PqF2c3gDcA/jXIWX8EbUL0MbAe6nb+AjotM+GSr1H5+vU79O/ALcB9gN+kOSBpZTf\nA08G/oPa/aF3D8tUV/Qf3f4ePt16+8zZPmif/TErurAALGd6My2zsyQvaOv9fFvuXdp2PSTJ1qWU\nq2axrNsAR1HPLXsBV1L/R2dqPXg0cFEp5aQO61irreO+1HPA6dTuPm+kXhR55cBHDgCOoB6XewHv\npna9WdaW90zq/tuPerV/PeD+bVmzdQTwJ+DfgD8AmwG7snoXC78F3EDtQvfxlnYP4Bjgv4FrgW2p\nwfJGwN4tz3Tfq7tSzx0HU4/R3wFvasu9OVCZwZpJ+qdv6l14SrIF9QLIJdRuZ5dS/0+/lOSfSim9\n/7O7tDK9HPhjW//rgCNZ8T872/PTdF4PnAi8oC3v2lmUdRTHVlp1pRRfvhbkBfwtcBpQ2usP1B+v\nOw/k26vN/xvgmdR+4fsN5PnHlmfPgfRntvQHtuknt+m7ten3U3+0nQ28sKXt0vLce5bLfn6b3r4v\nzxrAz1v6ljPsj4OB5dPMf0dbziYD+2XLNv0q4PIZ1nEs8IMh6b1lvW+Kcp3XN71ly/sLYI2+9Ie1\n9Of2pRVg34Hl9T6/1yzK1dvG+02xzDe09PsPrOO4gXyvaul36brPptiPJ7XvzFp9aXcH/grs35f2\nVloj0gzL+1Ar17od8s7pPug7Tm+dxbHvusxpjz31R9TFwHcH8j285fuPWS5v2/59MIvj+Uvgxx3z\nPqutY4eB9NdTz00bt+kdW75DBvJ9gPqjO33Tp8ywzvOAg4ek37xPqPdoFOBJq/B9Hnr8++ZfCHxj\ninmhXmx4PfVH+Bpdlzvw+T2Am4A7zpB/X1bUGf2vT/fl+Tj1h/gdBz57NHDqNMteq++796C+9GMZ\nfn7alyH/39P835zSO+6zKevqHFtfvkb1MnLVgim1xeFB1KuWb6NekXoycFSSNwz5yMupJ+aXlVLe\nNDBvF2rl/aXU7kZrtSuF32rzd2h/j6NWUr2RQR5JvYr8nYG0C0spv5rlsh8K/K709SMu9YrYzV2h\nVlPvcluZYv6JwIati8QTBrtJdPSVWeQ9tPR1NSul/JB6Ne8WI0bNod6+/vRAem968Ar41wemT29/\n79r+znqftRaqrYHPl76WoFLKucAPh5Rhrs31PlgVc7XMe1FbMD7Tn1hK+QFwPrPfl2dTr9J+JLVb\n1Baz/HwXu1DL9qMh54PeTdf9hu2rdandgqB+Bx+Y5L+TPDrJrVexXJcBvwHemeT5Se65issZJvSd\nd5JsmuQjSc6nnhv/Sg2Wb089ntMvLLldahewX1NbgP9KvQcoQNdyb09tKe693tg3bxdqK8IVA8fo\nKOABSW7XyrFOkteldk39SyvH99sy7sXcO6yUMnj+7lLWUR5baZUYQGhBlVJuLKV8r5TyhlLKo6lN\nyKcD+/T3Q212B35P7b40aGNgHeAqaiXQe13S5t+xre9y4GfATknuRL2a+9322rHl3alNz2rZ1Btg\nLx5StmFpq2ILamV9+bCZpZTjgKe3fF8BLk3y7ST3n8U6ZjPa0VTbutksljFbvW4dg+W8aGB+z+C+\nuq79vRWs8j7bkPpDZ9i+umhIGbrojR5ztw5553QfrKK5WuZU2wKrsC9LKVdQ/38vAD4I/Lb1z3/q\n9J/kd9SrxF1sTD1Ofx14ndDm33Eg/0z76pPUbikPof5ovDz13q6u5QFuvlfqMdTWsXcAZ7U+/f82\nm+UMavcl3Il2jFLv8zic2hX0rdQLLg+mXgSCbt+BTwAvonYdfEz7/Etm8XmAk0spJ/W9zu2btzGw\nJ7c8Ru9p83vH6B3UVoRPU0f/244VXU5X5/9jKsO+5zOWdVTHVlod3gOhsVJKuSD1JucDqFeiTuib\n/VTgo8CxSR5ZSum/AfsyareAf5xi0Rf0vf8utY/pTu1zp1FP7Bsn6Q2V+ZG+/F2XfSG1L++gTYak\nzUq7v+LRwPFlmpFHSimHAoe2vuA7Uu/Z+GaSzcvMowDB1K0bwwzbrk2oLUk911GDr36DP7Bmo/dj\n7M6sfD/BndvfKYdJncoq7LM/UvfTnYfMu/OqlIE6fOvbqPfZvHeGvHO+D0aky7Hv35ZBd6b+YJrN\n8iilnAo8tV3F3ZZ6X8UXkjyglHLGFGX9NvCYJNuUUk6eIk/PZcC51HtQhjlvhs8PlrdQzzcfaRdN\ndqZ+Bz5PDSqgnn9W2vYktwiuSim/AfZMvTngAcBLgQ8mOa+U8o3ZlKvPY6ldzX7Qprei7tdnlVJu\nbgVL8sQuC0sdeGI3aterA/rS/34VyzfMZdSWhHdNMb93zt4d+GQp5a195bjNLNZzbfvMOqWU6/vS\npzrHDTu/dirriI6ttMpsgdCCmaZ7wb3b38ERmn5P/YG3BnVoy/5RWr5JvWK0wcBVqd5rMIDYjHpD\n3LGluoR6r8KbqZXld1Zh2T8GtkhycxeGdrVuqh8anbQK493UK1Xv6/KZUspVpZQjqD9MNmVFhXYd\n9SbNufC0rDzqzMOAzan7oed8aitPv8cPWVbXch3X/g7eaPnM9vd7HZYx1DT7bDDf1cDJwNOz8qgz\nd6PerHvcsM/NsO4TqDelvi5TPAwrSW8Y11Hsg+uZu+9FT5djfya11WqlbUnyD9Sr/P37sut3CYBS\nyg2tO+EbqeeM+0yVF/gY9R6sD7QuaitJHXWpd6P7N6ktVldNcT74wzTrmVYp5Y+llM9Tuz32b+uw\nbX8CU2jntFOBV7Skwc92kvoQuXdTL458riX3ulj9tS/f2qz4/vUb9r1al3qOHRxoYa9VKeMUvkm9\nEf3nUxyjXivQrYeU49lDljfV+en89vfm/du6Qc5myN+uZQXm7thKq8sWCC2kM5J8l9p15FzgdtRR\nJV4EfKGU8tvBD5RSLkyyI/XH1rFJdiqlXFBKOTbJZ6lXkventlzcRO2WsCvwmrJilKfvUUdBeRQr\nms2hBhYvBX7brvb01tl12YdQRyD5cpLXUbs4vahtV1fr9AUgt2bFg+QeSr0ZcconGSfZj9oC8F3q\nVavNqaMAnVrq8xSg3vj84iT/Qr16fWUp5cxZlK/fbYHDknyEOvrKO6h90D/Zl+dzwBuSvB44ntqK\n84why+pUrlLKz9ux2LddYf4Rdd+8EfhsKeW02WxAx302zBupfduPSB0i9DbU4PMKZm5BmMqzqFfC\nT0zy36x4kNy9qaMdrU0drWxO90HzC+DxSb5JbWG5YCDoXhUzHvtSyo1J3kS9+v5paleSzaitMWdT\nu7p0Xl6SJ1BHuDmMek5Zn3o8r2TlwHYlpZTLWzenw4FT2v7vPUhuO+r/8aHU4/MZ6o/MY1KfD/Ez\nauvAVtSHQP5TKeWarjspyUf7yncJdXCJZ7HiHqveth+U5H3U0XgewMAP7tbt7gBqy8U51B/pe1FH\nUOryILfN2rlnDWrXse2pA0MEeGIp5S8t3y+pP5zfluRG6g/wqR6wN/R7leR44JVJLqQGbs9hbrs+\nvol6nv5ekg9QW4U2pP7YvkcppfdU6W9SRwI8nbrPnsLwH/9TnZ++Qf2f/98k+1CDo1dTu7vOWVnn\n4NhKc6+MwZ3cvibzRa2UD6dWRtdSn0T9U+oJeJ2+fHvRRmHqS9uYeq/EWcBmLW0N6lN/f9aWd0V7\n/25q60H/un9C30hLLa03QtPBQ8raadnUeziOBK6hjqxxALWl4+aRhKbZHwezYkSRm6g/Kn5JHaVj\n+yH59+pfLvVq7FHUq4XXUft1f5yVR8a5cyvfle2zx061jwfKdV7f9JYt74uB/dt2XkP9QX33gc/e\nqu2DC9s6P0/9QXbzyDkdy7VlX961qX2vz6f+eDm/Ta89pIzPGyjPji19x677bJrjtQv1R99f2vfh\nq8C9BvJ0GoWpL/9tqMNI/pT6/3Ad9Sr9AdQfE3O+D1raw6itKtey8sg+Ux37LsvsdOxb3j2o/0/X\nUbt0fArYdLbfJWrA/Xlq8HAt9bt5JPCQjvv/btRRkXo3915Fvcl5b+B2A2XZF/hVy3d5y7cvbWSu\nvn3y6Bn+b5dRR/m5pC3rXGpLY//61qD+0Dyf+r92FDVg6T9WG1MvYpzV8lxObcF5bIft7h/N6K/U\nH/U/oI7stdGQ/A9s86+hDpywH/A8bvm/OtX3akvqj+8r23Z/gPq/uNJ3aIqy7tvyrTVDvs2pLUu/\npwbiF1JHNtqjL8+dqMHZH9vrM9T7MTqdn9q8h7djf03b93vQ8f+ma1lX59j68jWqV28YOUmSJEma\nkfdASJIkSerMAEKSJElSZwYQkiRJkjozgJAkSZLUmQGEJEmSpM4MICRJkiR1ZgAhSZIkqTMDCEmS\nJEmdGUBIkiRJ6swAQkMlOTjJEXOwnH2TnDEXZZphPVsmKUm2HfW6Jl2SvZJcNaJlH5vkA33T5yV5\n1YjWNbLtkCbBfNYTc7Uujc4o6/vBuqDV908b0brm5XfLYmcAsQi0E+e+87zalwF79JVhpR92Y+h3\nwKbAqV0/kGTHJOfNkOe8dqLqf/1pNcs6uI4F37dtX/S276Ykf05yWpIDktx9IPvngXt0XO5sA7un\nAK+dTdk7lmNYZdN5O6RxZz0xd9rFhWNnyDNYL5QkneufjuUY2QWUWZRhr77tuzHJn5KclORtSTYe\nyP5fwCM6LrdX59ypY1EeDHxwNmXvUIap6qfO2zHJ1lroAmg8lVKuWOgyzEYp5UbgohEtfj/gQ33T\nN41oPastydqllL+uxiL+DrgcuA3wAODlwOlJHl9KOQ6glPIX4C+rXdg+SdYppVxfSrl8Lpc7nVFs\nhzRJFls9MQLPB/pbRVbn3DsySdYA0urJVXENsBUQ4HbUH/OvAZ6f5BGllF8ClFKuAua0Vbevbrh0\nLpc7nVFsx1JkC8QilGSdJG9Pcn6S65L8Jsl/tHlrJvl4knOT/CXJ2Ule3U4gvc8fnOSIJG9IcnGS\nq5J8Isl6g3l676nR+Ev6rkRs2WVdHbdn/SSfbOW4OMlrW/kO7suzR5ITk1yZ5JIkX0yyWd/8la4k\n9F3deFSSnyS5pl012XoVdvmVpZSL+l6X9K13gyQfbWW6Mslx/VczktwxyWeTLG/76OdJnt03f6p9\ne4urM9Ns465JTkhyPfDYNu+JSU5Ocm07Pm9Lsk6Hbb2kbeM5pZQvATsCPwUOSrJmW/ZKXX+SbJHk\nq0kub/v5V0l2b7PPbX9PbGU9trfd7Ri/JslyYHlLH3YF8zZJPt2+Hxdl4IpchrQupO/KXVa0Mn2x\n5T1v2Ha0tBcmOSfJ9e3v84es6wXt+3d1+9/bA2nMZInVE0O2b90k729luzbJ8Uke3jd/7SQHJrmg\nbf/vkryzb/5TUltZ/9LOXccl2WSWxfjTQN1wWd/yN0vyuSR/bK+vJ7ln3/yt2nnzonYuOSXJE/rm\nHwvcDXhPb3+29GHnrZXqi16eVjecAVwP3KfNe3aSX7R9dlaS/+xwLErbvgtLKWeWUj4NPBT4E/Dh\nvnKs1PUnyd8nOSa1RfvKJD9LslOSLYHvtmyXtrIf3NvuJB9K8l9JLgV+2NKHtcbcue3Xa9r3vL81\nbGjrQlauL6aqnwa3Y40kb2zfoeuSnJ5ktyHremqSo1t5fpHkMTPs10XNAGJxOgTYE3gF9aTwXOo/\nMtRj+nvgn9u81wOvA549sIxHUK8wPwp4KrAz8K4p1vcy4MfAJ6jdhDaldhnquq6ZvLeV58nAI1u5\n/nEgzzrAPm3eE4A7AZ/tsOx3AHsDWwOXAZ9JklmWb6i2nK8Dm7UyPQj4HvCdJJu2bLcCTmnz/w44\nAPhIkke1+VPt29l4F/AG4N7AT5I8FvgM8IG2zucATwPePtttbFes3kft6vOgKbJ9ELg1sFNb38tZ\n8X3crv3dhbptT+n73COA+7d5j2JqrwB+ST2G+wBvT/KUafIPenD7+/xWhgcPy5TkydR99n7gftRj\n9cEkTxzI+ibgq9Tv4uepwdXdZlEeaT4stXpi0LuBf6Ge3x4EnA58s+/c+x/UOmV34J4t75kASe4M\nfI66j+4D7AB8ajXLc7Mkt6b+QL6Wug8fClwIfLvNg9rK+w3gMdR9/CXgy0nu3eY/hXphZT9W7M/Z\nuBW1XnghcF/g/NQLIm+nnsPuA7yS2pLw4tluY7tK/2FghyQbTZHt/6jbvR31GO1L3Se/o36foNYZ\nm1K/Pz17UFs7/pH6HZ7Km4HDgQcCHwU+ORgwzGC6+qnfy4D/R91Xfw98hXqsHjiQ723AgdTjeSLw\nuSS3mUV5FpdSiq9F9KKeCAuwyyw+807g233TB1Mrktv0pe0BXAes35fniL75xwIfWIV17QucMU3+\n21Cvjuzel7Y+8Efg4Gk+d++2HzZv01u26W3b9I5t+rF9n3lY/2c67rvz2n65qu/1ujbvkW16vYHP\nnAq8epplfg742HT7tq/8d+pLm2obnzrw2e8BbxxI+6dW1kxRplusb8i+/uc2vRdwVd/804B9plju\nSmUe+A5eCqw7kL7Svmj7/+iBPB8DftA3XYCnDTlur5ohz+B2/BA4aEg5B9f1jr7ptajN+3t0/U75\n8jXqF0usnhhcF7WOuB7Ys2/+msCvgbe26QOBY4ad86gXIwpwt9XYx4XaBbK/bnhmm/cc4Oz+dbfy\nXdY7j06xzOOBN/RNr3Qea2krnbda2o70nb9bngJsM5Dvt8CzBtJeDvximjLdYn1983Zp69lu2HEE\n/gwsm+KzK5V54Dt02pD8K+2L9tn/HcjzbeDT7f2WDK97bq4LpskzuB2/B940pJyD63ph3/zNWtrD\nV/U7Nu4v74FYfB5E7YP/3akyJHkR8Dxq8+d6wNrA+QPZTiv1CkLPj6lX+bei/iDspOO6enn/kXrF\npeeFwBntMyf0EkspV2dgBITUrkf7UK803IF6dQLgrrTuL1Po35YL2t+NZ/jMoP2Bj/dN9/rpb0O9\n8n7pQKPGraj7kdRuP3tTr35tBqxL3c/HzmL9MzlpYHobYLskr+lLW4N6fO5MvSI0G72NK1PMPwD4\ncJJdqBX2V0opJ3dY7hmllOs65PvxkOnZtEB0dR/goIG0HwBPGki7+TtVSrmhNbMP3kwoLaQlVU+U\nUj4zkG2rtowf9hJKKTcm+TH1ajvUgONo4Kwk3wKOBL5RSrkJ+Bn1x+YZbd63gUPL7PvZ/z/gm33T\nF7e/2wB3B64cqBtuzYq6YX1qnfYE6tXvtal1R+f9OoMb6BtUpLUSbEFtAe+/p28tVpzjZ2umumF/\n4GNJllHrhi+VUn7VYbld6g8YXjc8vuNnO0lyO+Au9H3Xmh8Auw6kTfV7Y0kygFh8pv1HT/Iv1C4Y\nrwJ+RL0C8BJqU+7cFmT26zqJGgD0XEw7mTL1Cah3oj2KepJ/FnAJtQvT96mV2XT6b2rrrWO2Xfcu\nK6WcMyR9Deo2DHa3grovoO6bV1KbQE+nXqV6OzOfVHo3avcf77WnyHv1kHK9GfjikLyrciNar0L+\nzbCZpZSPJzmKejJ9NPCjJO8opew7w3IHy72qCrf8v5hqX3VZ1kxpgzdKFuwOqvGy1OqJWyy2/Z3y\n/7WUckrra78LtbX4EOBnSR7Tgo2dge2p3bKeC7wj9Ybgn3XfOi6apm44ldp9alDvAtR/tbK9itpa\ncQ3wSWau026i2/nuurLyTdO9c9SLqMdhLtyXur/PGzazlLJvks8Aj6Pen7dPkheVUgYv1Ayai7rh\nFnVoklWtF2CWdUMppbTgccnWDQYQi88p1C/kTqx85aPn4cBPSin9Y+lvNSTf3ydZv5TS+0fdntok\n/Osp1ns9tQl2VdYF3DzqzUon2yTnUP/ptqPd0NT6iN6vryz3pgYMryul9PKM4gr0bJ0CbALcVEoZ\n+uOauo++Vkr5FNx838TfsqIvMgzft70f+pv2vR/sbzldue49RcU2K60F5eXUYzHlEIWllOXUPqgf\nbS0fL6M2A1/fsgxu32xsP2T6l33Tl9LXPzj1RsjB/sJ/7VCGX1KPV3/l9nDgF7MprDQGllQ9McQ5\nbV0Pp13YaOeqh1L73feWdSX1QsoX2026xwN/A5xVaj+THwM/TrIf8HNqS/FsAoipnAI8A/hDKWWq\nYb8fDnyy1MEqSNJruT6rL89UdcOtk9yulNK7UDVj3VBKuTjJ74GtSimf7L4pw7W+/S8Cjpuu5aaU\ncjY1QDqwtXw8j3qOnau64aCB6V7d0F+H9gzupxnLUEr5c5ILqMfrO32zJr5uMIBYZEopZyf5ArVZ\n8GXUE9XmwJbtR+pZwF5JHkc9ye5OvYnrjwOLWot68+d+1Oa5d1L7E04V+Z9H7RazJfUq+uWzWNd0\n23NVkoOAdyX5A7V7zRuolV8vuv8ttd/tS5P8D7WryVu6rmOEvk1t1vxqklcDv6J2EdqF2r/3+9R9\n9C+po4P8Afh3atP2T/uWcx633LfnUG802zfJ3tQ+lm/oWK79gCOSnA98gdqUfT9qP9VXz/DZjZOs\nRb035f7Af1K7Q+xaphgCMMkB1C4HZ1GH+NuFFSfWS6j9hB+bOvrRtWX2Qz9un+S1wKHUfrN7As/s\nm/8d6sgvPwJupLbwXDuwjPOARyU5jnplbth39D3UHxonA99q2/FMRtNdShqZpVZPDNm+q9uP0Xe2\neuNc6rlqE9qzApK8glqfnEq9gPCv1NaP5Um2p7aWHkVt4XgQtXvPXP0g/Ay1ZeGrSd5ErcO2AHYD\nPtx+VJ8FPDnJV1v59qF2Yep3HvCPST5NPW/9AfgJ9Qr9O5K8j3rDbteboPcF/jv1WUZHUlsutgY2\nK6W8Y5rPpd14DrABK4Zx3YBbdvHsfWA9aivLF9t2bEILJluW86l1/OOTfA34y0B3uS6ekuREapfg\np1Fv9n8I1EA0yfHAa5L8upV1cBu71k/vAfZLcja1e9Ue1J4H28yyvEvKkm1aWeL2pF5lOZD6o/Vg\n6j8HwEeoPxr/jzoKwJbUUY4GHUe94vJd6ogC3wGm+3H5X9Ro/RfUyP6us1jXTF5F7Y50eCvPadRm\n7GsB2tWNZdQbgX9BPdG+YhXWM6faFaxdqfvuf6kjfHwBuBcr+j++lXp/xzeoNzdfTa1c+t1i35b6\nLIfdqaMf/YzaJel1Hct1FLVRkIAnAAAgAElEQVQf6E5t3SdQ78P4bYeP/5xa6f6UGoj8FLh/KeV7\n03xmDeC/W/mPplbIy1pZbqCOhvI86j75apdtGLA/NZj5KXV/vqmUcmjf/FdSr0IeSw0yPkatGBjI\nsxM1KPspQ5RSDqMGeP/ZtuVlwItLKV9bhTJLC22p1RODXtOW+wlqkHB/6k3jvXu8rqTeo3ACNYB6\nIPC4Uso1wBXUQTWOoF4dfy/wllKHJ11tbR07UM9LX6Tu/0OADVkROL2Cep76PrV+OL697/cmauDx\na9oV9VKflfNM6uhNpwMvAN7YsVwfo97g/SxqvfL99vlzZ/joran1wgXU/fkK4GvA/Up7BsQQN1K3\n9xBq3fgVaovPK1pZfk+ty99GrTNW5QGE+1JHczoN+Dfg2aWUE/vmP6f9PZH6PVzpItws6qcDqUHE\nu6n3bT6ZOnjJnD44cLFJ/Q2kSdKacu9USnnCTHkXQpJ1qVcn3lNKmYuKRpI0C+NeT0haWHZh0oJL\n8iBqt6QTgNtSryzdljrGviRJksbIgnVhSvKZJGcmOSPJQb2741MdmPoU2NPS9+TgJMtSn2J5dhsW\nrJe+TeqTAc9pn52TB4VpXr2C2rXkO9S+kju0G3MlTRjrB0kabyPrwpRkwyluVOzN35UVYz3/H/C9\nUsqHWvq/U/uWPwQ4oJTykCR3oPaL35Z6483J1Iek/DHJCdT+ysdTbww6sJTyDSRJY8f6QZIWt1G2\nQJyU5P+SPHLYFZ9SypGloXZd2bzN2o06tFkppRwP3D710fSPpT6R9vJW8RwN7NLm3a6U8uO2rE9S\nb7aVJI0n6wdJWsRGeQ/E31IfHvJS4H+SfAo4uJRyQX+m1jT9LOoVIqhP6/1dX5blLW269OVD0m8h\nyQuoIw6w/vrrb3Pve9971ht18mWXzSr/Nne846zXIUmjdPLJJ/+hlLLRAhZhrOqHuagbwPpB0uLX\ntX4YWQDRxow/gjoe/UbU8Xd/m+QfSikn9GX9ILV5ujd82bD+qcOeNDtT+rAyfZT6sCu23XbbctJJ\nJ3Xaln455JBZ5T9p2bKZM0nSPGrPCFkw41Y/zEXdANYPkha/rvXDSG+iTrJBu7JzOPWK03Op4/X2\n5u8DbMTKY/ovp4573LM5dXze6dI3H5IuSRpT1g+StHiNLIBoT048hfogrD1LKTuUUg4ppVzb5j+P\n2m/1GaWUm/o+ejiwZxttY3vgivZgmKOAnZNsmGRDYGfgqDbvyiTbt760e7JqD6uSJM0D6wdJWtxG\neQ/EF4C92pP+hvkw9WFhP2730H25lLIfdZSMXamPvL8GeDbUpy8meQv1iYIA+7UnMkJ9AuHBwHrU\nkTscYUOSxpf1gyQtYqO8B+LwGeYPXXcbKeMlU8w7CDhoSPpJwP1WoZiSpHlm/SBJi9uCPUhOkiRJ\n0uJjACFJkiSpMwMISZIkSZ0ZQEiSJEnqzABCkiRJUmcGEJIkSZI6M4CQJEmS1JkBhCRJkqTODCAk\nSZIkdWYAIUmSJKkzAwhJkiRJnRlASJIkSerMAEKSJElSZwYQkiRJkjozgJAkSZLUmQGEJEmSpM4M\nICRJkiR1ZgAhSZIkqTMDCEmSJEmdGUBIkiRJ6swAQpIkSVJnBhCSJEmSOjOAkCRJktSZAYQkSZKk\nzgwgJEmSJHVmACFJkiSpMwMISZIkSZ0ZQEiSJEnqzABCkiRJUmcGEJIkSZI6M4CQJEmS1JkBhCRJ\nkqTODCAkSZIkdWYAIUmSJKkzAwhJkiRJnRlASJIkSerMAEKSJElSZwYQkiRJkjozgJAkSZLUmQGE\nJEmSpM4MICRJkiR1ZgAhSZIkqTMDCEmSJEmdGUBIkiRJ6swAQpIkSVJnBhCSJEmSOjOAkCRJktSZ\nAYQkSZKkzgwgJEmSJHVmACFJkiSpMwMISZIkSZ0ZQEiSJEnqzABCkiRJUmcGEJIkSZI6M4CQJEmS\n1JkBhCRJkqTODCAkSZIkdWYAIUmSJKkzAwhJkiRJnRlASJIkSerMAEKSJElSZwYQkiRJkjozgJAk\nSZLU2YIFEEkOSnJJkjP60vZN8vskp7bXrn3zXpvknCRnJnlsX/ouLe2cJHvP93ZIkuaW9YMkjbeF\nbIE4GNhlSPr7SikPbK8jAZLcF9gd+Lv2mQ8mWTPJmsD/AI8D7gs8o+WVJC1eB2P9IElja62FWnEp\n5XtJtuyYfTfgc6WU64Bzk5wDbNfmnVNK+Q1Aks+1vL+Y4+JKkuaJ9YMkjbcFCyCm8dIkewInAa8s\npfwR2Aw4vi/P8pYG8LuB9IfMSyk7yiGHdM5bli0bYUkkadFbUvWDJC1W43YT9YeArYAHAhcC723p\nGZK3TJM+VJIXJDkpyUmXXnrp6pZVkjR/RlY/WDdI0uyMVQBRSrm4lHJjKeUm4H9Z0Qy9HNiiL+vm\nwAXTpE+1/I+WUrYtpWy70UYbzW3hJUkjM8r6wbpBkmZnrAKIJJv2TT4Z6I3AcTiwe5J1k9wduCdw\nAnAicM8kd0+yDvVGusPns8ySpNGzfpCk8bFg90Ak+SywI3CnJMuBfYAdkzyQ2sx8HvBCgFLKz5N8\ngXrz2w3AS0opN7blvBQ4ClgTOKiU8vN53hRJ0hyyfpCk8baQozA9Y0jyx6fJ/zbgbUPSjwSOnMOi\nSZIWkPWDJI23serCJEmSJGm8GUBIkiRJ6swAQpIkSVJnBhCSJEmSOjOAkCRJktSZAYQkSZKkzgwg\nJEmSJHVmACFJkiSpMwMISZIkSZ0ZQEiSJEnqzABCkiRJUmcGEJIkSZI6M4CQJEmS1JkBhCRJkqTO\nDCAkSZIkdWYAIUmSJKkzAwhJkiRJnRlASJIkSerMAEKSJElSZwYQkiRJkjozgJAkSZLU2YwBRJKH\nJVm/vd8jyf5J7jb6okmSxpn1gyRNpi4tEB8CrknyAODVwPnAJ0daKknSYmD9IEkTqEsAcUMppQC7\nAQeUUg4AbjvaYkmSFgHrB0maQGt1yHNlktcCewA7JFkTWHu0xZIkLQLWD5I0gbq0QPwLcB3w3FLK\nRcBmwHtGWipJ0mJg/SBJE2jGFohWKezfN/1b7OMqSRPP+kGSJtOUAUSSK4Ey1fxSyu1GUiJJ0liz\nfpCkyTZlAFFKuS1Akv2Ai4BPAQGeiTfJSdLEsn6QpMnW5R6Ix5ZSPlhKubKU8udSyoeAp466YJKk\nsWf9IEkTqEsAcWOSZyZZM8kaSZ4J3DjqgkmSxp71gyRNoC4BxL8C/wxc3F5Pb2mSpMlm/SBJE2ja\nUZjamN5PLqXsNk/lkSQtAtYPkjS5pm2BKKXcSH3CqCRJN7N+kKTJ1eVJ1D9M8gHg88DVvcRSyikj\nK5UkaTGwfpCkCdQlgPiH9ne/vrQCPHLuiyNJWkSsHyRpAnV5EvVO81EQSdLiYv0gSZNpxlGYkmyQ\nZP8kJ7XXe5NsMB+FkySNL+sHSZpMXYZxPQi4kjpU3z8DfwY+McpCSZIWBesHSZpAXe6B2KqU0v9k\n0TcnOXVUBZIkLRrWD5I0gbq0QPwlycN7E0keBvxldEWSJC0S1g+SNIG6tED8G3BIX7/WPwJ7jaxE\nkqTFwvpBkiZQl1GYTgUekOR2bfrPIy+VJGnsWT9I0mTqMgrT25PcvpTy51LKn5NsmOSt81E4SdL4\nsn6QpMnU5R6Ix5VS/tSbKKX8Edh1dEWSJC0S1g+SNIG6BBBrJlm3N5FkPWDdafJLkiaD9YMkTaAu\nN1F/GjgmySeAAjwHOGSkpZIkLQbWD5I0gbrcRP3uJKcBjwYCvKWUctTISyZJGmvWD5I0mbq0QAD8\nErihlPLtJLdOcttSypWjLJgkaVGwfpCkCdNlFKbnA4cCH2lJmwGHjbJQkqTxZ/0gSZOpy03ULwEe\nBvwZoJRyNrDxKAslSVoUrB8kaQJ1CSCuK6Vc35tIshb1ZjlJ0mSzfpCkCdQlgDguyeuA9ZI8Bvgi\n8LXRFkuStAhYP0jSBOoSQOwNXAqcDrwQOBJ4wygLJUlaFKwfJGkCdRnG9Sbgf9sLgCQPA344wnJJ\nksac9YMkTaYpA4gkawL/TB1V45ullDOSPAF4HbAe8KD5KaIkaZxYP0jSZJuuBeLjwBbACcCBSc4H\nHgrsXUpxmD5JmlzWD5I0waYLILYF7l9KuSnJrYA/AH9TSrlofoomSRpT1g+SNMGmu4n6+ta/lVLK\ntcBZVg6SJKwfJGmiTdcCce8kp7X3AbZq0wFKKeX+Iy+dJGkcWT9I0gSbLoC4z7yVQpK0mFg/SNIE\nmzKAKKWcP58FkSQtDtYPkjTZujxITpIkSZIAAwhJkiRJszBlAJHkmPb3XaNaeZKDklyS5Iy+tDsk\nOTrJ2e3vhi09SQ5Mck6S05Js3feZZS3/2UmWjaq8kqTR1w/WDZI03qZrgdg0ySOAJyV5UJKt+19z\ntP6DgV0G0vYGjiml3BM4pk0DPA64Z3u9APgQ1EoF2Ad4CLAdsE+vYpEkjcSo64eDsW6QpLE13ShM\nb6KeoDcH9h+YV4BHru7KSynfS7LlQPJuwI7t/SHAscBrWvonSykFOD7J7ZNs2vIeXUq5HCDJ0dSK\n57OrWz5J0lAjrR+sGyRpvE03CtOhwKFJ3lhKecs8lmmTUsqFrQwXJtm4pW8G/K4v3/KWNlW6JGkE\nFqh+sG6QpDExXQsEAKWUtyR5ErBDSzq2lHLEaIs1VIaklWnSb7mA5AXUJm7uete7zl3JJGkCjUn9\nYN0gSfNsxlGYkrwDeBnwi/Z6WUsblYtb8zPt7yUtfTmwRV++zYELpkm/hVLKR0sp25ZStt1oo43m\nvOCSNEnmuX6wbpCkMdFlGNfHA48ppRxUSjmI2of08SMs0+FAb7SMZcBX+9L3bCNubA9c0ZqzjwJ2\nTrJhu0Fu55YmSRqt+awfrBskaUzM2IWpuT1weXu/wVytPMlnqTe63SnJcuqIGe8EvpDkucBvgae3\n7EcCuwLnANcAzwYopVye5C3AiS3ffr2b5iRJIzfn9YN1gySNty4BxDuAnyb5LrVP6Q7Aa+di5aWU\nZ0wx61FD8hbgJVMs5yDgoLkokySps5HUD9YNkjTeutxE/dkkxwIPplYQrymlXDTqgkmSxpv1gyRN\npk5dmFp/0sNHXBZJ0iJj/SBJk6fLTdSSJEmSBBhASJIkSZqFaQOIJGskOWO+CiNJWhysHyRpck0b\nQJRSbgJ+lsRHc0qSbmb9IEmTq8tN1JsCP09yAnB1L7GU8qSRlUqStBhYP0jSBOoSQLx55KWQJC1G\n1g+SNIG6PAfiuCR3A+5ZSvl2klsDa46+aJKkcWb9IEmTacZRmJI8HzgU+EhL2gw4bJSFkiSNP+sH\nSZpMXYZxfQnwMODPAKWUs4GNR1koSdKiYP0gSROoSwBxXSnl+t5EkrWAMroiSZIWCesHSZpAXQKI\n45K8DlgvyWOALwJfG22xJEmLgPWDJE2gLgHE3sClwOnAC4EjgTeMslCSpEXB+kGSJlCXUZhuSnII\n8BNq0/SZpRSbqCVpwlk/SNJkmjGASPJ44MPAr4EAd0/ywlLKN0ZdOEnS+LJ+kKTJ1OVBcu8Fdiql\nnAOQZCvg64AVhCRNNusHSZpAXe6BuKRXOTS/AS4ZUXkkSYuH9YMkTaApWyCSPKW9/XmSI4EvUPu4\nPh04cR7KJkkaQ9YPkjTZpuvC9MS+9xcDj2jvLwU2HFmJJEnjzvpBkibYlAFEKeXZ81kQSdLiYP0g\nSZOtyyhMdwf+HdiyP38p5UmjK5YkadxZP0jSZOoyCtNhwMepTxe9abTFkSQtItYPkjSBugQQ15ZS\nDhx5SSRJi431gyRNoC4BxAFJ9gG+BVzXSyylnDKyUkmSFgPrB0maQF0CiL8HngU8khVN1KVNS5Im\nl/WDJE2gLgHEk4F7lFKuH3VhJEmLivWDJE2gLk+i/hlw+1EXRJK06Fg/SNIE6tICsQnwqyQnsnIf\nV4fpk6TJZv0gSROoSwCxz8hLIUlajKwfJGkCzRhAlFKOm4+CSJIWF+sHSZpMXZ5EfSV1VA2AdYC1\ngatLKbcbZcEkSePN+kGSJlOXFojb9k8n+Sdgu5GVSJK0KFg/SNJk6jIK00pKKYfhGN+SpAHWD5I0\nGbp0YXpK3+QawLasaLKWJE0o6wdJmkxdRmF6Yt/7G4DzgN1GUhpJ0mJi/SBJE6jLPRDPno+CSJIW\nF+sHSZpMUwYQSd40zedKKeUtIyiPJGnMWT9I0mSbrgXi6iFp6wPPBe4IWEFI0mSyfpCkCTZlAFFK\neW/vfZLbAi8Dng18DnjvVJ+TJC1t1g+SNNmmvQciyR2AVwDPBA4Bti6l/HE+CiZJGl/WD5I0uaa7\nB+I9wFOAjwJ/X0q5at5KJUkaW9YPkjTZpmuBeCVwHfAG4PVJeumh3iR3uxGXTZI0nqwfNBFyyCGd\n85Zly0ZYEmm8THcPxKyfUi1JWvqsHyRpslkJSJIkSerMAEKSJElSZwYQkiRJkjqbdhhXjbfZ3NwF\n3uAlSZKk1WcLhCRJkqTODCAkSZIkdWYAIUmSJKkzAwhJkiRJnRlASJIkSerMAEKSJElSZwYQkiRJ\nkjozgJAkSZLUmQGEJEmSpM4MICRJkiR1ZgAhSZIkqTMDCEmSJEmdGUBIkiRJ6swAQpIkSVJnBhCS\nJEmSOjOAkCRJktTZ2AYQSc5LcnqSU5Oc1NLukOToJGe3vxu29CQ5MMk5SU5LsvXCll6SNArWDZK0\n8MY2gGh2KqU8sJSybZveGzimlHJP4Jg2DfA44J7t9QLgQ/NeUknSfLFukKQFNO4BxKDdgEPa+0OA\nf+pL/2Spjgdun2TThSigJGneWTdI0jwa5wCiAN9KcnKSF7S0TUopFwK0vxu39M2A3/V9dnlLW0mS\nFyQ5KclJl1566QiLLkkaEesGSVpgay10AabxsFLKBUk2Bo5O8qtp8mZIWrlFQikfBT4KsO22295i\nviRp7Fk3SNICG9sWiFLKBe3vJcBXgO2Ai3vNz+3vJS37cmCLvo9vDlwwf6WVJM0H6wZJWnhjGUAk\nWT/JbXvvgZ2BM4DDgWUt2zLgq+394cCebcSN7YEres3ZkqSlwbpBksbDuHZh2gT4ShKoZfy/Uso3\nk5wIfCHJc4HfAk9v+Y8EdgXOAa4Bnj3/RZYkjZh1gySNgbEMIEopvwEeMCT9MuBRQ9IL8JJ5KJok\naYFYN0jSeBjLLkySJEmSxpMBhCRJkqTOxrILkyRJ0lzLIYfMnEnSjGyBkCRJktSZLRCSJEmrabat\nG2XZspkzSWPKFghJkiRJnRlASJIkSerMAEKSJElSZwYQkiRJkjozgJAkSZLUmQGEJEmSpM4MICRJ\nkiR1ZgAhSZIkqTMDCEmSJEmdGUBIkiRJ6swAQpIkSVJnBhCSJEmSOjOAkCRJktSZAYQkSZKkzgwg\nJEmSJHVmACFJkiSpMwMISZIkSZ0ZQEiSJEnqzABCkiRJUmcGEJIkSZI6M4CQJEmS1JkBhCRJkqTO\nDCAkSZIkdWYAIUmSJKkzAwhJkiRJnRlASJIkSerMAEKSJElSZwYQkiRJkjozgJAkSZLU2VoLXQBJ\nUpVDDplV/rJs2YhKIknS1GyBkCRJktSZAYQkSZKkzgwgJEmSJHXmPRBjZLb9nyVJkqT5ZguEJEmS\npM4MICRJkiR1ZgAhSZIkqTPvgZAkSYuS9w5KC8MWCEmSJEmdGUBIkiRJ6swAQpIkSVJnBhCSJEmS\nOjOAkCRJktSZozBJkiSNudmMOFWWLRthSSRbICRJkiTNgi0QkiRJ88xnWGgxswVCkiRJUmcGEJIk\nSZI6M4CQJEmS1JkBhCRJkqTODCAkSZIkdeYoTJoTsx1NwjGqJUmSFidbICRJkiR1ZgAhSZIkqTO7\nMEmSJC0hdivWqBlAaCifkClJkqRh7MIkSZIkqTMDCEmSJEmdLZkuTEl2AQ4A1gQ+Vkp55wIXSZI0\nBqwfFhe70Erjb0kEEEnWBP4HeAywHDgxyeGllF8sbMkkSQtpUusHb6KVNEpLIoAAtgPOKaX8BiDJ\n54DdgCVdQUiSZjS29cNsfuQv5h/4tigsPZPy3dXUlkoAsRnwu77p5cBDFqgsmmOjvpLmiVBa0qwf\n5pgBgWZjMbeGLeayj1pKKQtdhtWW5OnAY0spz2vTzwK2K6X8+0C+FwAvaJP3As5chdXdCfjDahR3\nsZiU7YTJ2Va3c+lZ1W29Wyllo7kuzDjqUj/MUd0Ak/Xdm4n7YgX3xQruixXGdV90qh+WSgvEcmCL\nvunNgQsGM5VSPgp8dHVWlOSkUsq2q7OMxWBSthMmZ1vdzqVnkrZ1NcxYP8xF3QAej37uixXcFyu4\nL1ZY7PtiqQzjeiJwzyR3T7IOsDtw+AKXSZK08KwfJGmOLYkWiFLKDUleChxFHabvoFLKzxe4WJKk\nBWb9IElzb0kEEACllCOBI+dhVavdzL1ITMp2wuRsq9u59EzStq4y64cF4b5YwX2xgvtihUW9L5bE\nTdSSJEmS5sdSuQdCkiRJ0jwwgJiFJLskOTPJOUn2XujydJFkiyTfTfLLJD9P8rKWfockRyc5u/3d\nsKUnyYFtG09LsnXfspa1/GcnWdaXvk2S09tnDkyS+d/Sm8uyZpKfJjmiTd89yU9amT/fbqIkybpt\n+pw2f8u+Zby2pZ+Z5LF96WNx/JPcPsmhSX7VjutDl+LxTPKf7Tt7RpLPJrnVUjmeSQ5KckmSM/rS\nRn4Mp1qHVt+4nB9GadTf28Ui81CvLhbtvHxCkp+1ffHmlj5n5+rFJiP8HTJWSim+OryoN9/9GrgH\nsA7wM+C+C12uDuXeFNi6vb8tcBZwX+DdwN4tfW/gXe39rsA3gADbAz9p6XcAftP+btjeb9jmnQA8\ntH3mG8DjFnB7XwH8H3BEm/4CsHt7/2Hg39r7FwMfbu93Bz7f3t+3Hdt1gbu3Y77mOB1/4BDgee39\nOsDtl9rxpD7861xgvb7juNdSOZ7ADsDWwBl9aSM/hlOtw9dqH8+xOT+MeDtH+r1dLC/moV5dLK+2\nTbdp79cGftK2cU7O1Qu9fau4T0byO2Sht+sW27nQBVgsr1YZH9U3/VrgtQtdrlXYjq8Cj6E+KGnT\nlrYpcGZ7/xHgGX35z2zznwF8pC/9Iy1tU+BXfekr5ZvnbdscOAZ4JHBEO7H9AVhr8BhSR2R5aHu/\nVsuXwePayzcuxx+4HfWHdQbSl9TxZMXTg+/Qjs8RwGOX0vEEtmTlH2IjP4ZTrcPXah/LBf8+zeO2\njuR7u9DbtZr7ZE7r1YXentXYD7cGTqE+6X1OztULvU2rsA9G9jtkobdt8GUXpu56P2h6lre0RaM1\njz2IeoVgk1LKhQDt78Yt21TbOV368iHpC+H9wKuBm9r0HYE/lVJuaNP9Zbt5e9r8K1r+2W7/fLsH\ncCnwidZE+rEk67PEjmcp5ffAfwG/BS6kHp+TWXrHs998HMOp1qHVM47fp/kyV9/bRWlE9eqi0rrs\nnApcAhxNvWI+V+fqxWaUv0PGigFEd8P6gS+aIayS3Ab4EvDyUsqfp8s6JK2sQvq8SvIE4JJSysn9\nyUOylhnmjfV2Uq9SbA18qJTyIOBqalP5VBbldra+w7tRm2/vAqwPPG5I1sV+PLtYytu2VLjPb2nJ\nfz9HWK8uKqWUG0spD6Refd8OuM+wbO3vkt0X8/A7ZKwYQHS3HNiib3pz4IIFKsusJFmbepL7TCnl\nyy354iSbtvmbUq8cwNTbOV365kPS59vDgCclOQ/4HLX58P3A7ZP0nnfSX7abt6fN3wC4nNlv/3xb\nDiwvpfykTR9KDSiW2vF8NHBuKeXSUspfgS8D/8DSO5795uMYTrUOrZ5x/D7Nl7n63i4qI65XF6VS\nyp+AY6n3QMzVuXoxGfXvkLFiANHdicA9293061BveDl8gcs0oyQBPg78spSyf9+sw4Fl7f0yah/O\nXvqebdSI7YErWlPsUcDOSTZsV4d3pvbjuxC4Msn2bV179i1r3pRSXltK2byUsiX12HynlPJM4LvA\n01q2we3sbf/TWv7S0ndvoyPcHbgn9YbUsTj+pZSLgN8luVdLehTwC5bY8aR2Xdo+ya1bOXrbuaSO\n54D5OIZTrUOrZxy/T/NlTr63813o1THqenVeNmKOJNkoye3b+/WoF39+ydydqxeNefgdMl4W+iaM\nxfSijqRwFrV/3+sXujwdy/xwatPXacCp7bUrtZ/dMcDZ7e8dWv4A/9O28XRg275lPQc4p72e3Ze+\nLXBG+8wHGLjBdwG2eUdWjH5wD+o/3jnAF4F1W/qt2vQ5bf49+j7/+rYtZ9I3AtG4HH/ggcBJ7Zge\nRh29Y8kdT+DNwK9aWT5FHZFiSRxP4LPUezv+Sr3a9Nz5OIZTrcPXnBzTsTg/jHgbR/q9XSwv5qFe\nXSwv4P7AT9u+OAN4U0ufs3P1Ynwxot8h4/TySdSSJEmSOrMLkyRJkqTODCAkSZIkdWYAIUmSJKkz\nAwhJkiRJnRlASJIkSerMAEJaDUmOTfLYgbSXJ/ngNJ+5avQlkyQtJOsHLWUGENLq+Sz1gTH9dm/p\nkqTJZf2gJcsAQlo9hwJPSLIuQJItgbsApyY5JskpSU5PstvgB5PsmOSIvukPJNmrvd8myXFJTk5y\nVJJN52NjJElzxvpBS5YBhLQaSimXUZ8guUtL2h34PPAX4MmllK2BnYD3JkmXZSZZG/hv4GmllG2A\ng4C3zXXZJUmjY/2gpWythS6AtAT0mqm/2v4+Bwjw9iQ7ADcBmwGbABd1WN69gPsBR7c6ZU3gwrkv\ntiRpxKwftCQZQEir7zBg/yRbA+uVUk5pTc0bAduUUv6a5DzgVgOfu4GVWwF78wP8vJTy0NEWW5I0\nYtYPWpLswiStplLKVcCx1Kbk3s1xGwCXtMphJ+BuQz56PnDfJOsm2QB4VEs/E9goyUOhNlkn+btR\nboMk6f+3c8coCMRAGEb/AY/owcQ7iGBh4zUERRAES29hExtBsJpiRZT3ykBgtxo+EjI984F/5QQC\nprFOss3rxY1Vkl1V7SUm4XQAAABkSURBVJMck1zeN4wxblW1SXJKck1yeK7fq2qeZPkcHLMkiyTn\nj/8FAFMzH/g7Ncb49jcAAAA/whUmAACgTUAAAABtAgIAAGgTEAAAQJuAAAAA2gQEAADQJiAAAIA2\nAQEAALQ9AGaz6XodUMKrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ee95373c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from IPython.display import display \n",
    "import visuals as vs\n",
    "\n",
    "# Split the data into features and target label\n",
    "income_raw = data['income']\n",
    "features_raw = data.drop('income', axis = 1)\n",
    "display(features_raw.head(n=5))\n",
    "\n",
    "# Visualize skewed continuous features of original data\n",
    "vs.distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing:\n",
    "Data processing involves cleaning and reformating of data like checking for invalid entries or missing entries. \n",
    "Dataset provided has no such invalid or missing records.\n",
    "\n",
    "Transforming Skewed Continuous Features:\n",
    "Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. \n",
    "The skewness value can be positive or negative, or undefined.The best way to fix it is to perform a log transform of the same data, with the intent to reduce the skewness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_level</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>7.684784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>9th</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Jamaica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>9.552866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>8.552367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass education_level  education-num  \\\n",
       "0   39          State-gov       Bachelors           13.0   \n",
       "1   50   Self-emp-not-inc       Bachelors           13.0   \n",
       "2   38            Private         HS-grad            9.0   \n",
       "3   53            Private            11th            7.0   \n",
       "4   28            Private       Bachelors           13.0   \n",
       "5   37            Private         Masters           14.0   \n",
       "6   49            Private             9th            5.0   \n",
       "7   52   Self-emp-not-inc         HS-grad            9.0   \n",
       "8   31            Private         Masters           14.0   \n",
       "9   42            Private       Bachelors           13.0   \n",
       "\n",
       "           marital-status          occupation    relationship    race  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black   \n",
       "5      Married-civ-spouse     Exec-managerial            Wife   White   \n",
       "6   Married-spouse-absent       Other-service   Not-in-family   Black   \n",
       "7      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "8           Never-married      Prof-specialty   Not-in-family   White   \n",
       "9      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "\n",
       "       sex  capital-gain  capital-loss  hours-per-week  native-country  \n",
       "0     Male      7.684784           0.0            40.0   United-States  \n",
       "1     Male      0.000000           0.0            13.0   United-States  \n",
       "2     Male      0.000000           0.0            40.0   United-States  \n",
       "3     Male      0.000000           0.0            40.0   United-States  \n",
       "4   Female      0.000000           0.0            40.0            Cuba  \n",
       "5   Female      0.000000           0.0            40.0   United-States  \n",
       "6   Female      0.000000           0.0            16.0         Jamaica  \n",
       "7     Male      0.000000           0.0            45.0   United-States  \n",
       "8   Female      9.552866           0.0            50.0   United-States  \n",
       "9     Male      8.552367           0.0            40.0   United-States  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAF2CAYAAAD+y36TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIkqbX/Dw7mqWDp/pY1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ee95b056d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log-transform the skewed features\n",
    "\n",
    "#In binary classification case skewed data means that one class is vastly more represented in the data than the other class.\n",
    "#skewed datasets could lead to models that are biased towards to the majority labels.\n",
    "#To mitigate this problem we are taking logarithm of the data to get normally distributed data\n",
    "import matplotlib.pyplot as plt\n",
    "skewed = ['capital-gain', 'capital-loss']\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n",
    "display(features_log_transformed.head(n=10))\n",
    "\n",
    "# Visualize the new log distributions\n",
    "vs.distribution(features_log_transformed, transformed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_level</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.667492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.452055</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.493151</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age          workclass education_level  education-num  \\\n",
       "0  0.301370          State-gov       Bachelors       0.800000   \n",
       "1  0.452055   Self-emp-not-inc       Bachelors       0.800000   \n",
       "2  0.287671            Private         HS-grad       0.533333   \n",
       "3  0.493151            Private            11th       0.400000   \n",
       "4  0.150685            Private       Bachelors       0.800000   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  \n",
       "0      0.667492           0.0        0.397959   United-States  \n",
       "1      0.000000           0.0        0.122449   United-States  \n",
       "2      0.000000           0.0        0.397959   United-States  \n",
       "3      0.000000           0.0        0.397959   United-States  \n",
       "4      0.000000           0.0        0.397959            Cuba  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import sklearn.preprocessing.StandardScaler\n",
    "#Transforms features by scaling each feature to a given range.\n",
    "#This estimator scales and translates each feature individually such that it is in the given range on the training set, \n",
    "#i.e. between zero and one.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize a scaler, then apply it to the featuresscaler\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_log_minmax_transform.head(n = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "Name: income, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 Total number of features after one-hot encoding is.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Private</th>\n",
       "      <th>workclass_ Self-emp-inc</th>\n",
       "      <th>workclass_ Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_ Portugal</th>\n",
       "      <th>native-country_ Puerto-Rico</th>\n",
       "      <th>native-country_ Scotland</th>\n",
       "      <th>native-country_ South</th>\n",
       "      <th>native-country_ Taiwan</th>\n",
       "      <th>native-country_ Thailand</th>\n",
       "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_ United-States</th>\n",
       "      <th>native-country_ Vietnam</th>\n",
       "      <th>native-country_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.667492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0  0.301370       0.800000      0.667492           0.0        0.397959   \n",
       "1  0.452055       0.800000      0.000000           0.0        0.122449   \n",
       "2  0.287671       0.533333      0.000000           0.0        0.397959   \n",
       "3  0.493151       0.400000      0.000000           0.0        0.397959   \n",
       "4  0.150685       0.800000      0.000000           0.0        0.397959   \n",
       "\n",
       "   workclass_ Federal-gov  workclass_ Local-gov  workclass_ Private  \\\n",
       "0                       0                     0                   0   \n",
       "1                       0                     0                   0   \n",
       "2                       0                     0                   1   \n",
       "3                       0                     0                   1   \n",
       "4                       0                     0                   1   \n",
       "\n",
       "   workclass_ Self-emp-inc  workclass_ Self-emp-not-inc  \\\n",
       "0                        0                            0   \n",
       "1                        0                            1   \n",
       "2                        0                            0   \n",
       "3                        0                            0   \n",
       "4                        0                            0   \n",
       "\n",
       "              ...              native-country_ Portugal  \\\n",
       "0             ...                                     0   \n",
       "1             ...                                     0   \n",
       "2             ...                                     0   \n",
       "3             ...                                     0   \n",
       "4             ...                                     0   \n",
       "\n",
       "   native-country_ Puerto-Rico  native-country_ Scotland  \\\n",
       "0                            0                         0   \n",
       "1                            0                         0   \n",
       "2                            0                         0   \n",
       "3                            0                         0   \n",
       "4                            0                         0   \n",
       "\n",
       "   native-country_ South  native-country_ Taiwan  native-country_ Thailand  \\\n",
       "0                      0                       0                         0   \n",
       "1                      0                       0                         0   \n",
       "2                      0                       0                         0   \n",
       "3                      0                       0                         0   \n",
       "4                      0                       0                         0   \n",
       "\n",
       "   native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
       "0                                0                              1   \n",
       "1                                0                              1   \n",
       "2                                0                              1   \n",
       "3                                0                              1   \n",
       "4                                0                              0   \n",
       "\n",
       "   native-country_ Vietnam  native-country_ Yugoslavia  \n",
       "0                        0                           0  \n",
       "1                        0                           0  \n",
       "2                        0                           0  \n",
       "3                        0                           0  \n",
       "4                        0                           0  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: One-hot encode the 'features_raw' data using pandas.get_dummies()\n",
    "\n",
    "# one hot encoding is done for the proper representation of the distinct elements of the variable.\n",
    "# It involves assigning 1 to working feature and 0’s to other idle features to produce a balanced matrix, \n",
    "# which is easy to understand during complex computations inside algorithms. \n",
    "# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\n",
    "features_final = pd.get_dummies(features_raw)\n",
    "\n",
    "# TODO: Encode the 'income_raw' data to numerical values\n",
    "income = income_raw.apply(lambda x: 1 if x == \">50K\" else 0)\n",
    "\n",
    "display(income.head(n=10))\n",
    "\n",
    "# Number of features after one-hot encoding\n",
    "features_encoded = list(features_final.columns)\n",
    "\n",
    "\n",
    "print (\"{} Total number of features after one-hot encoding is.\".format(len(features_encoded)))\n",
    "\n",
    "display(features_final.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in Training set is 36177 \n",
      "Total number of samples in Testing set is 9045 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ck288\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split\n",
    "# We need to split the data into random train and test subsets\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
    "                                                    income, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Total number of samples in Training set is {} \".format(X_train.shape[0]))\n",
    "print (\"Total number of samples in Testing set is {} \".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "Naive Predictor Performance\n",
    "\n",
    "TN (True Negative) : Number of correct predictions that an instance is irrelevant\n",
    "FP (False Positive) : Number of incorrect predictions that an instance is relevant\n",
    "FN (False Negative) : Number of incorrect predictions that an instance is irrelevant\n",
    "TP (True Positive) : Number of correct predictions that an instance is relevant\n",
    "Accuracy(ACC) – The proportion of the total number of predictions that were correct.\n",
    "Therefore, a model's ability to precisely predict those that make more than \\$50,000 is more important than the model's ability to recall those individuals. \n",
    "We can use F-beta score as a metric that considers both precision and recall:\n",
    "fscore = (1+beta**2)*(accuracy*recall)/(beta**2*accuracy+recall)\n",
    "\n",
    "\n",
    "The 'accuracy' and 'fscore'for the naive predictor as calculated as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11208\n",
      "34014\n",
      "Naive Predictor Result: [Accuracy score is : 0.2478, F-score is: 0.2917]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Sum of total number of ones in 'income' gives the number of people with income 50K because we are dealing with naive case.\n",
    "# This sum is the value of true positive TP\n",
    "TP = np.sum(income) \n",
    "print (TP)    # true positive count\n",
    "\n",
    "FP = income.count() - TP # Subtracting false positive from income.count gives the value of false positive FP\n",
    "print (FP)    # false positive count\n",
    "\n",
    "\n",
    "#False negatives Predictions that should be true but were predicted as false\n",
    "FN = 0 # Not considering false negative in naive case\n",
    "TN = 0 # Not considering true negative in naive case\n",
    "\n",
    "\n",
    "# accuracy, precision and recall calculation\n",
    "\n",
    "\n",
    "\n",
    "pred_income=income.apply(lambda x:1) #here lambda is created as anonymous function that runs on the fly \n",
    "\n",
    "\n",
    "TP=sum(map(lambda x,y: 1 if x==1 and y==1 else 0, income,pred_income)) #True Positive\n",
    "FP=sum(map(lambda x,y: 1 if x==0 and y==1 else 0, income,pred_income)) #False Positive\n",
    "FN=sum(map(lambda x,y: 1 if x==1 and y==0 else 0, income,pred_income)) #False Negative\n",
    "\n",
    "#accuracy is a performance measure that is a ratio of correctly predicted observation to the total observations.\n",
    "accuracy = float(TP)/(TP+FP)\n",
    "\n",
    "#Recall is the ratio of correctly predicted positive observations to the all observations in actual class \n",
    "recall = float(TP)/(TP+FN)\n",
    "\n",
    "#precision = None\n",
    "\n",
    "# TODO: Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.\n",
    "beta=0.5\n",
    "\n",
    "#F-score is the harmonic mean of precision and recall:\n",
    "fscore = (1+beta**2)*(accuracy*recall)/(beta**2*accuracy+recall)\n",
    "\n",
    "# Print the results \n",
    "print (\"Naive Predictor Result: [Accuracy score is : {:.4f}, F-score is: {:.4f}]\".format(accuracy, fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Training and Predicting Pipeline is implemented as below:\n",
    "\n",
    "In machine learning a framework for converting raw data to data usable by ML algorithm, training an ML algorithm, and finally using the output of the ML algorithm to perform actions in the real-world is the pipeline.A training and predicting pipeline\n",
    "reduces the time taken to train the models effectively on any size of training data and perform predictions on the testing data. \n",
    "\n",
    "1.Accuracy score calculation for both training subset and testing set.\n",
    "2.F-score calculation for both training subset and testing set.\n",
    "3.Learner is fit on training data(X_train, y_train) and total prediction time is recorded\n",
    "4.Prediction is performed on the testing set(X_test)\n",
    "5.beta parameter is set to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train[:int(sample_size)],y_train[:int(sample_size)])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end-start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set(X_test),\n",
    "    #       then get predictions on the first 300 training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    \n",
    "    print(\"predictions_test results are:\")\n",
    "    print(predictions_test)\n",
    "    \n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # Get end time\n",
    "\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "    \n",
    "    print(\"Total training time is:\")\n",
    "    print(results['train_time'])\n",
    "    \n",
    "    print(\"Total prediction time is:\")\n",
    "    print(results['pred_time'])\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples which is y_train[:300]\n",
    "    results['acc_train'] = accuracy_score(y_train[:300],predictions_train)\n",
    "    \n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(results['acc_train'])\n",
    "        \n",
    "    # TODO: Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_test,predictions_test)\n",
    "    \n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(results['acc_test'])\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples using fbeta_score()\n",
    "    results['f_train'] = fbeta_score(y_train[:300],predictions_train,beta=0.5)\n",
    "        \n",
    "    # TODO: Compute F-score on the test set which is y_test\n",
    "    results['f_test'] = fbeta_score(y_test,predictions_test,beta=0.5)\n",
    "       \n",
    "    # Success\n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Model application\n",
    "List three of the supervised learning models above that are appropriate for this problem that you will test on the census data. For each model chosen\n",
    "Describe one real-world application in industry where the model can be applied.\n",
    "What are the strengths of the model; when does it perform well?\n",
    "What are the weaknesses of the model; when does it perform poorly?\n",
    "What makes this model a good candidate for the problem, given what you know about the data?\n",
    "\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "According to me the 3 supervised learning models suitable for this situation are:\n",
    "\n",
    "1.Gaussian Naive Bayes (GaussianNB):\n",
    "\n",
    "a. Pros/Strengths of Naive Bayes:\n",
    "In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.Naive Bayes classifier is computationally fast, simple to implement and highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem.As the whole dataset is too big to fit in memory at once, Partial fit is done several times consecutively on different chunks of a dataset so as to implement online learning.\n",
    "\n",
    "b. Cons/Weaknesses of Naive Bayes:\n",
    "The disadvantage of with Naive-Bayes is that if you have no occurrences of a class label and a certain attribute value together (e.g. class=\"nice\", shape=\"sphere\") then the frequency-based probability estimate will be zero.This problem happens when we are drawing samples from a population and the drawn vectors are not fully representative of the population.\n",
    "(ref:https://en.wikipedia.org/wiki/Naive_Bayes_classifier, \n",
    "     http://www.statsoft.com/textbook/naive-bayes-classifier)\n",
    "     \n",
    "c. Real world application in industry:\n",
    "Naive Bayes clasiifier is used to support the assessment of individual risk of relapse or progression in patients diagnosed with brain tumour undergoing RT postoperatively.\n",
    "(ref:http://www.sciencedirect.com/science/article/pii/S0167814007005221)\n",
    "\n",
    "d. The reason for choosing Naive Bayes classifier:\n",
    "   when we have a look at our dataset we that there are features who seem to be independent or having no relationship with the earnings of a person like relationship,race,sex,native-country, and workclass. Naive Bayes algorithm which has a fundamental assumption that each feature makes an independent and equal contribution to the outcome is well suited for this situation.\n",
    "\n",
    "\n",
    "\n",
    "2.SVC\n",
    "\n",
    "a. Pros/Strengths of SVC:\n",
    "In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.\n",
    "Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier.It performs similarly to logistic regression when linear separation and can handle high dimensional data well.\n",
    "\n",
    "b. Cons/Weaknesses of SVC:\n",
    "Disadvantage of SVM classifier is that it is susceptible to overfitting/training issues depending on kernel.\n",
    "(ref:https://en.wikipedia.org/wiki/Support_vector_machine, \n",
    "     http://www.statsoft.com/textbook/support-vector-machines)\n",
    "     \n",
    "c. Real world application in industry:\n",
    "SVC classifier is used in text (and hypertext) categorization to classify documents into predefined set of categories or handling and organizing online information.\n",
    "(ref:http://140.123.102.14:8080/reportSys/file/paper/604410151/604410151_2_paper.pdf)\n",
    "\n",
    "d. The reason for choosing SVC:\n",
    "The dataset we are currently dealing has multiple driving features which can dealt very well with SVC. As SVC is Effective in high dimensional spaces where number of dimensions is greater than the number of samples.SVC is basically a two-class classifier. Multi-class problems are solved  in a one-against-rest fashion. \n",
    "\n",
    "   \n",
    "\n",
    "3.AdaBoost classifier\n",
    "\n",
    "a. Pros/Strengths of Adaboost classifier:\n",
    "AdaBoost (Adaptive Boosting) is a powerful classifier that works well on both basic and more complex recognition problems. AdaBoost works by creating a highly accurate classifier by combining many relatively weak and inaccurate classifiers. Therefore acts as a meta algorithm, which allows you to use it as a wrapper for other classifiers\n",
    "\n",
    "b. Cons/Weaknesses of Adaboost classifier:\n",
    "However the disadvantage of AdaBoost is that it can be sensitive to noisy data and outliers. In some problems it can be susceptible to the overfitting problem than most learning algorithms.\n",
    "(ref:https://en.wikipedia.org/wiki/AdaBoost, \n",
    "     http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf)\n",
    "\n",
    "c. Real world application in industry:\n",
    "Detecting faces in an image using Viola-Jones face detector.The Viola-Jones face detector uses a “rejection cascade” consisting of many layers of classifiers. If at any layer the detection window is _not _recognized as a face, it’s rejected and we move on to the next window. \n",
    "(ref:http://mccormickml.com/2013/12/13/adaboost-tutorial/)\n",
    "\n",
    "d. The reason for choosing Adaboost classifier:\n",
    "Analysing the census dataset given we can see that there are a few features which are not directly related or predict an individuals income which seem to be the weak features for prediction.In such case AdaBoosting is an approach to machine learning based on the idea of creating a highly accurate prediction rule by combining many relatively weak and inaccurate\n",
    "rules. It combines multiple “weak classifiers” into a single “strong classifier”. Thus Adaboost classifier is best suited for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation: Initial Model Evaluation\n",
    "\n",
    "In the code cell, you will need to implement the following:\n",
    "Import the three supervised learning models you've discussed in the previous section.\n",
    "Initialize the three models and store them in 'clf_A', 'clf_B', and 'clf_C'.\n",
    "Use a 'random_state' for each model you use, if provided.\n",
    "Note: Use the default settings for each model — you will tune one specific model in a later section.\n",
    "Calculate the number of records equal to 1%, 10%, and 100% of the training data.\n",
    "Store those values in 'samples_1', 'samples_10', and 'samples_100' respectively.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "In the code cell below the three supervised learning models GaussianNB, SVC and AdaBoostClassifier are imported from sklearn and initialized.\n",
    "The training data(X_train) is spilt into samples for 1%, 10%, and 100% of training data.\n",
    "As the whole dataset is too big to fit in memory at once, Partial fit is done several times consecutively on different chunks of a dataset as 'samples_1', 'samples_10', and 'samples_100'.\n",
    "Train each model on the same training set to obtain the metrics like total training time, prediction time, accuracy on training set and testing set for each of the 3 models used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_test results are:\n",
      "[1 1 1 ..., 1 1 1]\n",
      "Total training time is:\n",
      "0.0015037059783935547\n",
      "Total prediction time is:\n",
      "0.03459572792053223\n",
      "Accuracy on training set:\n",
      "0.4\n",
      "Accuracy on testing set:\n",
      "0.351796572692\n",
      "GaussianNB trained on 361.77 samples.\n",
      "predictions_test results are:\n",
      "[1 1 1 ..., 1 1 1]\n",
      "Total training time is:\n",
      "0.011997699737548828\n",
      "Total prediction time is:\n",
      "0.03509330749511719\n",
      "Accuracy on training set:\n",
      "0.383333333333\n",
      "Accuracy on testing set:\n",
      "0.366058595909\n",
      "GaussianNB trained on 3617.7 samples.\n",
      "predictions_test results are:\n",
      "[1 1 1 ..., 1 1 1]\n",
      "Total training time is:\n",
      "0.14134812355041504\n",
      "Total prediction time is:\n",
      "0.0726931095123291\n",
      "Accuracy on training set:\n",
      "0.593333333333\n",
      "Accuracy on testing set:\n",
      "0.59767827529\n",
      "GaussianNB trained on 36177 samples.\n",
      "predictions_test results are:\n",
      "[0 0 0 ..., 0 0 0]\n",
      "Total training time is:\n",
      "0.020554065704345703\n",
      "Total prediction time is:\n",
      "0.4894413948059082\n",
      "Accuracy on training set:\n",
      "0.76\n",
      "Accuracy on testing set:\n",
      "0.756218905473\n",
      "SVC trained on 361.77 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ck288\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_test results are:\n",
      "[0 0 0 ..., 0 0 1]\n",
      "Total training time is:\n",
      "1.2380750179290771\n",
      "Total prediction time is:\n",
      "2.85833740234375\n",
      "Accuracy on training set:\n",
      "0.833333333333\n",
      "Accuracy on testing set:\n",
      "0.832614704256\n",
      "SVC trained on 3617.7 samples.\n",
      "predictions_test results are:\n",
      "[0 0 0 ..., 0 0 1]\n",
      "Total training time is:\n",
      "131.41514611244202\n",
      "Total prediction time is:\n",
      "20.728824615478516\n",
      "Accuracy on training set:\n",
      "0.853333333333\n",
      "Accuracy on testing set:\n",
      "0.837147595357\n",
      "SVC trained on 36177 samples.\n",
      "predictions_test results are:\n",
      "[0 0 0 ..., 0 0 1]\n",
      "Total training time is:\n",
      "0.07518959045410156\n",
      "Total prediction time is:\n",
      "0.10277223587036133\n",
      "Accuracy on training set:\n",
      "0.893333333333\n",
      "Accuracy on testing set:\n",
      "0.820674405749\n",
      "AdaBoostClassifier trained on 361.77 samples.\n",
      "predictions_test results are:\n",
      "[0 0 0 ..., 0 0 1]\n",
      "Total training time is:\n",
      "0.25516605377197266\n",
      "Total prediction time is:\n",
      "0.10280561447143555\n",
      "Accuracy on training set:\n",
      "0.84\n",
      "Accuracy on testing set:\n",
      "0.849861802101\n",
      "AdaBoostClassifier trained on 3617.7 samples.\n",
      "predictions_test results are:\n",
      "[0 0 0 ..., 0 0 1]\n",
      "Total training time is:\n",
      "2.0683834552764893\n",
      "Total prediction time is:\n",
      "0.10578203201293945\n",
      "Accuracy on training set:\n",
      "0.85\n",
      "Accuracy on testing set:\n",
      "0.857600884467\n",
      "AdaBoostClassifier trained on 36177 samples.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "clf_GNB = GaussianNB()        #Can perform online updates to model parameters via partial_fit method.\n",
    "clf_SVC = SVC(random_state=0)\n",
    "clf_ADAB = AdaBoostClassifier(random_state=0)\n",
    "\n",
    "# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "samples_1 = len(X_train)/100\n",
    "samples_10 = len(X_train)/10\n",
    "samples_100 = len(X_train)\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "\n",
    "#As the whole dataset is too big to fit in memory at once,\n",
    "#Partial fit is done several times consecutively on different chunks of a dataset so as to implement online learning.\n",
    "        \n",
    "for clf in [clf_GNB, clf_SVC, clf_ADAB]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    \n",
    "    results[clf_name] = {}\n",
    "    \n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "       \n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "#print(results, accuracy, fscore)\n",
    "        \n",
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "#vs.evaluate(results, accuracy, fscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Choosing the Best Model\n",
    "\n",
    "Based on the evaluation you performed earlier, in one to two paragraphs, explain to CharityML which of the three models you believe to be most appropriate for the task of identifying individuals that make more than \\$50,000.\n",
    "\n",
    "Which model has the highest score? Your answer should include discussion of the:\n",
    "metrics - F score on the testing when 100% of the training data is used\n",
    "prediction/training time\n",
    "the algorithm's suitability for the data.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "Based on metrics and prediction/training time:\n",
    "\n",
    "Oberving the results from the above code cell for the metrics like training time, prediction time, accuracy on training set and accuracy on testing set, it can be seen that AdaBoostClassifier with prediction time = 0.0625, accuracy on training set = 0.85, accuracy on testing set = 0.8576 performs better than Naive Bayes and SVC with the same training and testing set provided. Thus AdaBoostClassifier is the most appropriate model for the task.\n",
    "\n",
    "\n",
    "Based on algorithm's suitability for the data:\n",
    "\n",
    "Analysing the census dataset given we can see that there are a few features which are not directly related or predict an individuals income which seem to be the weak features for prediction.\n",
    "As AdaBoost can select informative features from a potentially very large feature pool, automatically finding good features for classification. Instead, one just needs to define a list of possibly informative features, and AdaBoost will choose those that are actually informative.It sequentially selects weak classifiers (i.e., ones that do not perform perfectly when used on their own) from a candidate pool and weights each of them based on their error. A weak learner is any statistical classifier that performs better than pure chance. \n",
    "In my opinion Adaboost classifier best suits this situation.\n",
    "\n",
    "(ref:https://en.wikipedia.org/wiki/AdaBoost, \n",
    "     http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Describing the Model in Layman's Terms\n",
    "\n",
    "In one to two paragraphs, explain to CharityML, in layman's terms, how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical jargon, such as describing equations.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "To begin with the dataset, it has about 13 features describing the earning of an individually and hence can predict in an individual can be a potential donor for charity as follows:\n",
    "\n",
    "1.age - A younger individual has higher possibility of being a donor. \n",
    "2.workclass - Better the workclass higher the possibility of being a donor.\n",
    "3.education_level - Higher the education_level higher the possibility of being a donor.\n",
    "4.education-num - Higher the education_num higher the possibility of being a donor.\n",
    "5.marital-status - An individual Never-married can be a potential donor.\n",
    "6.occupation - An individual with occupation of Exec-managerial has higher the possibility of being a donor.\n",
    "7.relationship - A person Not-in-family can be a potential donor.\n",
    "8.race - Sometimes the race of an individual can also have an influence on the individual earnings and hence can conclude if an individual can be a donor.\n",
    "9.sex - Sometimes the sex of an individual can also have an influence on the individual earnings and hence can conclude if an individual can be a donor.\n",
    "10.capital-gain -  Higher the capital-gain higher the possibility of an individual being a donor.\n",
    "11.capital-loss - Higher the capital-loss lower the possibility of an individual being a donor.\n",
    "12.hours-per-week - Higher the hours-per-week higher the possibility of an individual being a donor.\n",
    "13.native-country - Sometimes the native-country can also have an influence on the individual earnings and hence can conclude if an individual can be a donor.\n",
    "\n",
    "Above, we’ve defined multiple rules to classify an individual to be a donor or not. But these rules individually are not strong enough to successfully classify all the individuals.Therefore, these rules are called as weak learner/base learner.\n",
    "\n",
    "To convert weak learner to strong learner, we’ll combine the prediction of each weak learner using methods like:\n",
    "•   Using average/ weighted average\n",
    "•   Considering prediction has higher vote\n",
    "\n",
    "Boosting Algorithms combines weak learners to form a strong rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
    "\n",
    "For choosing the right distribution, here are the following steps:\n",
    "\n",
    "Step 1:  The weak/base learner takes all the distributions and assign equal weight or attention to each observation.\n",
    "\n",
    "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n",
    "\n",
    "Adaptive Boosting works on similar method as discussed above. It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.\n",
    "\n",
    "(ref:https://www.quora.com/What-is-AdaBoost, \n",
    "     https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tuning:\n",
    "\n",
    "Fine tune the chosen model. Use grid search (GridSearchCV) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following:\n",
    "Import sklearn.grid_search.GridSearchCV and sklearn.metrics.make_scorer.\n",
    "Initialize the classifier you've chosen and store it in clf.\n",
    "Set a random_state if one is available to the same state you set before.\n",
    "Create a dictionary of parameters you wish to tune for the chosen model.\n",
    "Example: parameters = {'parameter' : [list of values]}.\n",
    "Note: Avoid tuning the max_features parameter of your learner if that parameter is available!\n",
    "Use make_scorer to create an fbeta_score scoring object (with $\\beta = 0.5$).\n",
    "Perform grid search on the classifier clf using the 'scorer', and store it in grid_obj.\n",
    "Fit the grid search object to the training data (X_train, y_train), and store it in grid_fit.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "In the abstract sense of machine learning, tuning is learning from variable data based on some parameters which have been \n",
    "identified to affect system performance as evaluated by some appropriate metric. Improved performance reveals which \n",
    "parameter settings are more favorable (tuned) or less favorable (untuned).\n",
    "In simple terms, tuning is essentially selecting the best parameters for an algorithm to optimize itsperformance given a \n",
    "working environment such as hardware, specific workloads, etc. And tuning in machine learning is an \n",
    "automated process for doing this.\n",
    "\n",
    "Following are the steps in model tuning:\n",
    "\n",
    "1.Import GridSearchCV, make_scorer, fbeta_score, AdaBoostClassifier libraries from sklearn \n",
    "2.Initialize an AdaBoostClassifier and create a dictionary of parameters 'n_estimators' and 'learning_rate'.\n",
    "3.Import sklearn.grid_search.GridSearchCV and sklearn.metrics.make_scorer.\n",
    "4.Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of   algorithm parameters specified in a grid. \n",
    "GridSearchCV applies cross-validation to select from a set of parameter values.\n",
    "5.Calculate the accuracy anf F score for unoptimised and optimised models.\n",
    "\n",
    "(ref: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ck288\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\ck288\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ck288\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ck288\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ck288\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ck288\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Unoptimized model\n",
      "------\n",
      "Accuracy score on testing data: 0.8576\n",
      "F-score on testing data: 0.7246\n",
      "\n",
      " For the Optimized Model\n",
      "------\n",
      "Final accuracy score on the testing data: 0.8677\n",
      "Final F-score on the testing data: 0.7452\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = AdaBoostClassifier(random_state=0)\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune\n",
    "#parameters = {'n_estimators':[75,100,200]}\n",
    "parameters = {'n_estimators':[75,200,500],'learning_rate':[1.0,1.5,2.0]}\n",
    "\n",
    "# TODO: Make an fbeta_score scoring object\n",
    "scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "# TODO: Perform grid search on the classifier using 'scorer' as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters,scoring=scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print (\"For the Unoptimized model\\n------\")\n",
    "print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\n",
    "print (\"\\n For the Optimized Model\\n------\")\n",
    "print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Final Model Evaluation\n",
    "\n",
    "What is your optimized model's accuracy and F-score on the testing data?\n",
    "Are these scores better or worse than the unoptimized model?\n",
    "How do the results from your optimized model compare to the naive predictor benchmarks you found earlier in Question 1?\n",
    "\n",
    "Answer:\n",
    "\n",
    "The accuracy and F1 score of the optimized, unoptimized models on testing data are as follows:\n",
    "\n",
    "    Metric\t        Benchmark Predictor\t    Unoptimized Model\t Optimized Model\n",
    "    Accuracy Score\t0.2478\t                0.8576\t             0.8677\n",
    "    F-score\t        0.2917\t                0.7246\t             0.7452\n",
    "   \n",
    "Looking at the scores from the above table of the metrics, the Optimized Model with an accuracy score = 0.8677 and  F-score = 0.7452 looks like performing better than Unoptimized Model(Accuracy Score = 0.8576, F-score = 0.7246)\n",
    "\n",
    "Also the results from the optimized model compared to the naive predictor benchmarks found earlier in Question 1 , \n",
    "from the scores, I can conclude that both the optimized model performs significantly better than the naive predictor benchmarks(Accuracy Scor = 0.2478, F-score = 0.2917). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance:\n",
    "    \n",
    "An important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than \\$50,000.\n",
    "\n",
    "Choose a scikit-learn classifier (e.g., adaboost, random forests) that has a feature_importance_ attribute, which is a function that ranks the importance of features according to the chosen classifier. In the next python cell fit this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset.\n",
    "\n",
    "\n",
    "Answer: \n",
    "\n",
    "I choose GradientBoostingClassifier from scikit-learn that has a feature_importance_ attribute. \n",
    "In the code cell numbered 12, GradientBoosting classifier has been trained on the entire training set(X_train, y_train). \n",
    "Also the normalised weights for first 5 most predictive features have been plotted for better visualisation.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question 6: Feature Relevance Observation\n",
    "\n",
    "When Exploring the Data, it was shown there are thirteen available features for each individual on record in the census data. Of these thirteen records, which five features do you believe to be most important for prediction, and in what order would you rank them and why?\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "Among all the thirteen available features in the dataset, I believe that the five most important features starting with the highly important ones are as follows:\n",
    "\n",
    "1.capital gain\n",
    "2.capital loss\n",
    "3.age\n",
    "4.hours per week\n",
    "5.education num\n",
    "\n",
    "I would rank capital gain and capital loss at the top because:\n",
    "1.capital gain directly referes to the profit earned by the individual whereas capital loss refers to the financial losses incurred by the individual. Thus these 2 features are important in the prediction.\n",
    "(ref : https://en.wikipedia.org/wiki/Capital_gain, \n",
    "       https://en.wikipedia.org/wiki/Capital_loss)\n",
    "2.Age is a factor that might not seem relevant at the very first time but when combined age with the marital status can give a lot information. For example: There are greater chances that a person younger in age, never married and earning at most 50K can contribute to charity.\n",
    "3.Hour per week and education num also decide an idividuals income thus need to be considered for prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation - Extracting Feature Importance:\n",
    "    \n",
    "Choose a scikit-learn supervised learning algorithm that has a feature_importance_ attribute availble for it. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm.\n",
    "\n",
    "In the code cell below, you will need to implement the following:\n",
    "Import a supervised learning model from sklearn if it is different from the three used earlier.\n",
    "Train the supervised model on the entire training set.\n",
    "Extract the feature importances using '.feature_importances_'.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "In the code cell below, GradientBoostingClassifier model has been imported from sklearn which is different from the three supervised learning models used earlier.\n",
    "\n",
    "GradientBoostingClassifier is then trained on on the entire training set(X_train, y_train).\n",
    "\n",
    "GradientBoostingClassifier has a feature_importance_ attribute availble using which we are extracting the feature importances and plotting the same using vs.feature_plot as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAFgCAYAAAArYcg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ee9695c128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Import a supervised learning model that has 'feature_importances_'\n",
    "\n",
    "# TODO: Train the supervised model on the training set \n",
    "\n",
    "#GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=0,n_estimators=500).fit(X_train, y_train)\n",
    "\n",
    "# TODO: Extract the feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Plot\n",
    "vs.feature_plot(importances, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Extracting Feature Importances\n",
    "\n",
    "Observe the visualization created above which displays the five most relevant features for predicting if an individual makes at most or above \\$50,000.\n",
    "How do these five features compare to the five features you discussed in Question 6? \n",
    "If you were close to the same answer, how does this visualization confirm your thoughts? If you were not close, why do you think these features are more relevant?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Feature Selection:\n",
    "I had expected capital gain, capital loss, age, hours-per-week, education number and marital status to be the important predictive features to indicate income level of an individual. \n",
    "\n",
    "From the five features discussed in Question 6 and the above plot for Normalized weights, I can conclude that my predictions were almost right except that the marital status was not found to be much relevant feature in prediction.\n",
    "\n",
    "From the above plot I can conclude that capital gain, capital loss, age, hours per week, education num are the features having significant weights.\n",
    "\n",
    "So this visualization confirms my thoughts that the income of an individual are greatly infulenced by factors like finacial gains/losses, number of hours of work, age and also qualification which are important features to predict a person to be donor or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection\n",
    "\n",
    "How does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower — at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of all features present in the data. This hints that we can attempt to reduce the feature space and simplify the information required for the model to learn. The code cell below will use the same optimized model you found earlier, and train it on the same training set with only the top five important features.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "It is not advisable to use all the available features in the data. It is not necessary that all the available data is useful.\n",
    "\n",
    "Feature selection needs to be done because of the following reasons:\n",
    "\n",
    "1.It enables the machine learning algorithm to train faster.\n",
    "2.It reduces the complexity of a model and makes it easier to interpret.\n",
    "3.It improves the accuracy of a model if the right subset is chosen.\n",
    "4.It reduces overfitting.\n",
    "\n",
    "The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable.\n",
    "\n",
    "As you can see in the above plot of 'Normalised weights for first five most predictive features' the 5 features capital gain, capital loss, age, hours per week, education num play a vital role in predicting the outcome.\n",
    "carry higher weights in predicting the outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model trained on full data\n",
      "------\n",
      "Accuracy on testing data: 0.8677\n",
      "F-score on testing data: 0.7452\n",
      "\n",
      "Final Model trained on reduced data\n",
      "------\n",
      "Accuracy on testing data: 0.8421\n",
      "F-score on testing data: 0.7003\n"
     ]
    }
   ],
   "source": [
    "# Import functionality for cloning a model\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Reduce the feature space\n",
    "X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "\n",
    "# Train on the \"best\" model found from grid search earlier\n",
    "clf = (clone(best_clf)).fit(X_train_reduced, y_train)\n",
    "\n",
    "# Make new predictions\n",
    "reduced_predictions = clf.predict(X_test_reduced)\n",
    "\n",
    "# Report scores from the final model using both versions of data\n",
    "print (\"Final Model trained on full data\\n------\")\n",
    "print (\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\n",
    "print (\"\\nFinal Model trained on reduced data\\n------\")\n",
    "print (\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions)))\n",
    "print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Question 8: Effects of Feature Selection\n",
    "\n",
    "How does the final model's F-score and accuracy score on the reduced data using only five features compare to those same scores when all features are used?\n",
    "If training time was a factor, would you consider using the reduced data as your training set?\n",
    "\n",
    "A. Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons: 1.simplification of models to make them easier to interpret by researchers/users 2.shorter training times, 3.to avoid the curse of dimensionality, 4.enhanced generalization by reducing overfitting Thus reducing the data to only five features also reduces the F-score and accuracy of the predictions.\n",
    "\n",
    "B. Increased training data has practical concerns like memory and processor time. It is not always necessary to have a huge dataset as this might not be much useful if additional training data is noisy. So it is the situation of either more data or better algorithms. So if training time was a factor, I would consider using the reduced data but with a much better algorithm. (ref: https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
